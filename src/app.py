# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EPZH1ItqawXd9EjXxtwKMEy7AzG48WqL
"""

import os
import json
import tensorflow as tf
import numpy as np
import pandas as pd
import joblib
import psycopg2
from fastapi import FastAPI
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

BASE_DIR = os.path.dirname(os.path.dirname(__file__))

# --- Model setup ---
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(20,)),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid"),
])

# IMPORTANT: set these filenames to exactly what exists in your project
MODEL_WEIGHTS_FILE = "credit_weights.weights.h5"   # adjust if filename differs
ARTIFACTS_FILE = "preprocess.pkl"                  # adjust if filename differs

model.load_weights(os.path.join(BASE_DIR, "models", MODEL_WEIGHTS_FILE))

# --- Preprocessing artifacts ---
artifacts = joblib.load(os.path.join(BASE_DIR, "artifacts", ARTIFACTS_FILE))
cat_cols = artifacts["cat_cols"]
num_cols = artifacts["num_cols"]
encoders = artifacts["encoders"]
scaler = artifacts["scaler"]

# --- Database connection ---
conn = psycopg2.connect(
    host="credit-db",
    database="credit_logs",
    user="suresh",
    password="secret123"
)
conn.autocommit = True

# --- FastAPI app ---
app = FastAPI(title="Credit API", version="0.4.0")

class InputSchema(BaseModel):
    account_status: str
    credit_history: str
    purpose: str
    savings: str
    employment: str
    personal_status: str
    guarantors: str
    property: str
    other_installments: str
    housing: str
    job: str
    phone: str
    foreign_worker: str
    months: int
    credit_amount: float
    installment_rate: int
    residence: int
    age: int
    credit_cards: int
    dependents: int
    true_label: Optional[int] = None
    sensitive_attributes: Optional[Dict[str, str]] = None


# --- Helper for safe category transform ---
def safe_transform_category(series, encoder):
    classes = list(getattr(encoder, "classes_", []))
    idx_map = {cls: i for i, cls in enumerate(classes)}
    return series.map(lambda x: idx_map.get(x, -1)).astype(int)


# --- Helper: preprocess ---
def preprocess(raw_inputs: List[dict]):
    df = pd.DataFrame(raw_inputs)

    # Drop fields not used for model training
    drop_cols = ["true_label", "sensitive_attributes"]
    for col in drop_cols:
        if col in df.columns:
            df = df.drop(columns=[col])

    # Transform categorical safely
    for c in cat_cols:
        df[c] = df[c].astype(str)
        enc = encoders[c]
        try:
            df[c] = enc.transform(df[c])
        except Exception:
            df[c] = safe_transform_category(df[c], enc)

    # Scale numerical
    df[num_cols] = scaler.transform(df[num_cols])

    # Keep transformed dicts for logging
    transformed_dicts = [df.iloc[i].to_dict() for i in range(len(df))]

    return df.values.astype(np.float32), transformed_dicts

# --- Schema endpoints ---
@app.get("/schema")
def schema():
    return {
        "categorical_columns": cat_cols,
        "numerical_columns": num_cols,
        "total_expected_features": len(cat_cols) + len(num_cols)
    }

@app.get("/categories")
def categories():
    return {c: list(encoders[c].classes_) for c in cat_cols}


# --- Predict endpoint ---
@app.post("/predict")
def predict(inputs: List[InputSchema]):
    raw_inputs = [i.dict() for i in inputs]

    X, transformed_dicts = preprocess(raw_inputs)
    probs = model.predict(X).ravel()
    classes = (probs >= 0.5).astype(int)

    results = []
    with conn.cursor() as cur:
        for i, raw in enumerate(raw_inputs):
            record = {
                "input": raw,
                "probability": float(probs[i]),
                "class": int(classes[i])
            }
            results.append(record)

            # Log raw prediction
            cur.execute(
                "INSERT INTO predictions (input_data, probability, predicted_class, true_label, sensitive_attributes) "
                "VALUES (%s, %s, %s, %s, %s::jsonb)",
                [
                    json.dumps(raw),
                    record["probability"],
                    record["class"],
                    raw.get("true_label"),
                    json.dumps(raw.get("sensitive_attributes")) if raw.get("sensitive_attributes") else None
                ]
            )

            # Log transformed features (store as JSONB)
            cur.execute(
                "INSERT INTO feature_logs (features) VALUES (%s::jsonb)",
                [json.dumps(transformed_dicts[i])]
            )

    return {"results": results}

# --- Drift helpers ---
def psi_for_bins(baseline_probs, current_probs):
    eps = 1e-9
    psi = 0.0
    for b, c in zip(baseline_probs, current_probs):
        if b < eps or c < eps:
            continue
        psi += (b - c) * np.log((b + eps) / (c + eps))
    return float(psi)


def ensure_json_list(x):
    if x is None:
        return None
    if isinstance(x, list):
        return x
    if isinstance(x, str):
        try:
            val = json.loads(x)
            return val if isinstance(val, list) else None
        except Exception:
            return None
    return None


def ensure_json_dict(x):
    if x is None:
        return None
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            val = json.loads(x)
            return val if isinstance(val, dict) else None
        except Exception:
            return None
    return None


# --- Drift compute ---
@app.post("/drift/compute")
def compute_drift(window_hours: int = 24):
    with conn.cursor() as cur:
        cur.execute(
            "SELECT features FROM public.feature_logs WHERE timestamp >= NOW() - INTERVAL %s",
            [f"{window_hours} hours"]
        )
        rows = cur.fetchall()
        if not rows:
            return {"message": "No feature logs in window", "window_hours": window_hours}

        feature_values = {}
        for features_json, in rows:
            try:
                fdict = json.loads(features_json) if isinstance(features_json, str) else features_json
            except Exception:
                continue
            if not isinstance(fdict, dict):
                continue
            for k, v in fdict.items():
                feature_values.setdefault(k, []).append(v)

        cur.execute("SELECT feature_name, feature_type, bins, distribution FROM public.baseline_stats")
        baselines = cur.fetchall()
        window_end = datetime.utcnow()
        window_start = window_end - timedelta(hours=window_hours)

        results = []
        for fname, ftype, bins_json, dist_json in baselines:
            bins = ensure_json_list(bins_json) if ftype == "numerical" else None
            base_dist = ensure_json_list(dist_json) if ftype == "numerical" else ensure_json_dict(dist_json)
            values = feature_values.get(fname, [])

            if not values or base_dist is None or (ftype == "numerical" and bins is None):
                continue

            if ftype == "numerical":
                counts = [0] * len(base_dist)
                for val in values:
                    idx = 0
                    while idx < len(bins) and val >= bins[idx]:
                        idx += 1
                    if idx >= len(counts):
                        idx = len(counts) - 1
                    counts[idx] += 1
                total = max(sum(counts), 1)
                current_probs = [c / total for c in counts]
                metric_value = psi_for_bins(base_dist, current_probs)
                metric_name = "PSI"
                threshold = 0.2
                status = "alert" if metric_value > 0.2 else ("warn" if metric_value > 0.1 else "ok")
                details = {"current_probs": current_probs}
            else:
                classes = list(base_dist.keys())
                counts = dict.fromkeys(classes, 0)
                for val in values:
                    sval = str(val)
                    if sval in counts:
                        counts[sval] += 1
                total = max(sum(counts.values()), 1)
                current_probs = {k: counts[k] / total for k in classes}
                metric_value = float(sum(abs(base_dist[k] - current_probs.get(k, 0.0)) for k in classes))
                metric_name = "freq_shift"
                threshold = 0.3
                status = "alert" if metric_value > 0.3 else ("warn" if metric_value > 0.15 else "ok")
                details = {"current_probs": current_probs}

            cur.execute(
                "INSERT INTO public.drift_metrics (window_start, window_end, feature_name, metric_name, metric_value, threshold, status, details) "
                "VALUES (%s, %s, %s, %s, %s, %s, %s, %s::jsonb)",
                [window_start, window_end, fname, metric_name, metric_value, threshold, status, json.dumps(details)]
            )
            results.append({"feature": fname, "metric": metric_name, "value": metric_value, "status": status})

    return {"window_hours": window_hours, "computed": results}

# --- Drift latest ---
@app.get("/drift/latest")
def drift_latest(limit: int = 20):
    with conn.cursor() as cur:
        cur.execute(
            "SELECT window_start, window_end, feature_name, metric_name, metric_value, threshold, status, details, created_at "
            "FROM public.drift_metrics ORDER BY created_at DESC LIMIT %s",
            [limit]
        )
        rows = cur.fetchall()

    out = []
    for r in rows:
        out.append({
            "window_start": str(r[0]),
            "window_end": str(r[1]),
            "feature_name": r[2],
            "metric_name": r[3],
            "metric_value": r[4],
            "threshold": r[5],
            "status": r[6],
            "details": r[7],
            "created_at": str(r[8])
        })
    return {"metrics": out}


# --- Drift plot ---
@app.get("/drift/plot")
def drift_plot(feature: str, limit: int = 50):
    with conn.cursor() as cur:
        cur.execute(
            "SELECT created_at, metric_value, metric_name, threshold, status "
            "FROM public.drift_metrics WHERE feature_name = %s "
            "ORDER BY created_at DESC LIMIT %s",
            [feature, limit]
        )
        rows = cur.fetchall()

    if not rows:
        return {"message": f"No drift metrics found for feature '{feature}'"}

    rows = rows[::-1]
    times = [r[0] for r in rows]
    values = [r[1] for r in rows]
    metric_name = rows[0][2]
    threshold = rows[0][3]
    statuses = [r[4] for r in rows]

    plt.figure(figsize=(9, 4.5))
    plt.plot(times, values, marker="o", label=f"{feature} ({metric_name})")
    plt.axhline(y=threshold, color="red", linestyle="--", label=f"threshold={threshold}")

    colors = {"ok": "green", "warn": "orange", "alert": "red"}
    for t, v, s in zip(times, values, statuses):
        plt.scatter([t], [v], color=colors.get(s, "blue"), zorder=3)

    plt.xticks(rotation=45)
    plt.xlabel("Time")
    plt.ylabel("Metric value")
    plt.title(f"Drift trend for {feature}")
    plt.legend()
    plt.tight_layout()

    plot_path = f"/tmp/{feature}_drift.png"
    plt.savefig(plot_path)
    plt.close()
    return FileResponse(plot_path, media_type="image/png")

# --- Performance compute ---
@app.post("/performance/compute")
def performance_compute(window_hours: int = 24):
    with conn.cursor() as cur:
        # ðŸ‘‡ Replace this query
        cur.execute(
            "SELECT predicted_class, true_label FROM public.predictions "
            f"WHERE created_at >= NOW() - INTERVAL '{window_hours} hours' AND true_label IS NOT NULL"
        )
        rows = cur.fetchall()

    if not rows:
        return {"message": "No predictions with true labels in window", "window_hours": window_hours}

    y_pred = [r[0] for r in rows]
    y_true = [r[1] for r in rows]


    tp = sum(1 for yp, yt in zip(y_pred, y_true) if yp == 1 and yt == 1)
    tn = sum(1 for yp, yt in zip(y_pred, y_true) if yp == 0 and yt == 0)
    fp = sum(1 for yp, yt in zip(y_pred, y_true) if yp == 1 and yt == 0)
    fn = sum(1 for yp, yt in zip(y_pred, y_true) if yp == 0 and yt == 1)

    accuracy = (tp + tn) / max(len(y_true), 1)
    precision = tp / max(tp + fp, 1)
    recall = tp / max(tp + fn, 1)
    f1 = 2 * precision * recall / max(precision + recall, 1)

    metrics = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "window_hours": window_hours,
        "counts": {"tp": tp, "tn": tn, "fp": fp, "fn": fn}
    }

    with conn.cursor() as cur:
        for name, value in [("accuracy", accuracy), ("precision", precision), ("recall", recall), ("f1", f1)]:
            threshold = 0.7 if name == "accuracy" else 0.5
            status = "alert" if value < threshold else ("warn" if value < threshold + 0.1 else "ok")
            details = {"counts": {"tp": tp, "tn": tn, "fp": fp, "fn": fn}}
            cur.execute(
                "INSERT INTO public.performance_metrics (window_start, window_end, metric_name, metric_value, threshold, status, details) "
                "VALUES (%s, %s, %s, %s, %s, %s, %s::jsonb)",
                [datetime.utcnow() - timedelta(hours=window_hours), datetime.utcnow(), name, value, threshold, status, json.dumps(details)]
            )

    return {"metrics": metrics}


# --- Performance latest ---
@app.get("/performance/latest")
def performance_latest(limit: int = 20):
    with conn.cursor() as cur:
        cur.execute(
            "SELECT window_start, window_end, metric_name, metric_value, threshold, status, details, created_at "
            "FROM public.performance_metrics ORDER BY created_at DESC LIMIT %s",
            [limit]
        )
        rows = cur.fetchall()

    out = []
    for r in rows:
        out.append({
            "window_start": str(r[0]),
            "window_end": str(r[1]),
            "metric_name": r[2],
            "metric_value": r[3],
            "threshold": r[4],
            "status": r[5],
            "details": r[6],
            "created_at": str(r[7])
        })
    return {"metrics": out}